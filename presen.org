#+title: 
#+author: Masataro Asai
#+include: "head.org"
#+LINK: img file:img/%s
#+LINK: png file:img/%s.png
#+LINK: jpg file:img/%s.jpg


#+BEGIN_outline-text-1
#+BEGIN_CENTER
#+BEGIN_XLARGE
Classical Planning in a

Deep Latent Space
#+END_XLARGE

(WIP project idea)
#+END_CENTER

#+BEGIN_NOTE
#+BEGIN_ALIGNRIGHT
Made by guicho2.71828 (Masataro Asai)
#+END_ALIGNRIGHT
#+END_NOTE
#+END_outline-text-1

* Latent Space $L$ of an original state space $S$

#+BEGIN_QUOTE
Apparently very complex $S$ can be described by low-dimentional $L$
#+END_QUOTE

[[jpg:static/cicada]]

* Manifold Hypothesis

#+BEGIN_QUOTE
$x$: high dimensional vector in $S$, $z$ in manifold $L$

*Real-world data is concentrated around a low dimensional manifold*

There is *a practical algorithm that finds a representation Z* of that manifold 
#+END_QUOTE

http://www.deeplearningbook.org/ and VAE tutorial

[[png:static/manifold]]

* Manifold Hypothesis

[[png:static/manifold2]]

* How to obtain $L$?

#+BEGIN_LARGER
Traditional Method: PCA (Principal Component Analysis)

*/✘/* Assumes a linear function
#+END_LARGER

\[y = f(x) = Wx + b\]

* Neural Network

A framework for learning a *function* that maps input $x$ to output $y$

*Non-linear* activation function (e.g. sigmoid, ReLU)

*NN w/ $>2$ layers can learn arbitrary function* (given enough neurons)

\[x = \sigma (Wx + b) \]

[[png:static/nn]]

* NN + PCA = AutoEncoder (AE)

#+BEGIN_CENTER
Unsupervised learning method which learns to 

*compress S* into *L* and *decompress back to S*
#+END_CENTER

#+BEGIN_CONTAINER-FLUID
#+BEGIN_ROW-FLUID
#+BEGIN_SPAN3

#+END_SPAN3
#+BEGIN_SPAN6
[[png:static/autoenc]]
#+END_SPAN6
#+BEGIN_SPAN3

#+END_SPAN3
#+END_ROW-FLUID
#+END_CONTAINER-FLUID

#+BEGIN_ALIGNRIGHT
→ equivalent to *PCA applied to nonlinear function*
#+END_ALIGNRIGHT

* Deep Autoencoder

Deep AE made available by various techniques e.g.

Stacked AE, pretraining, +CNN+, +dropout+, +Batch-Normalization+, +GPU+

[[png:static/deep-ae]]

* Recap

#+BEGIN_CENTER
#+BEGIN_LARGER
+ Real world can be compactly represented by (compressed into) a latent space $L$ 
+ Latent space can be obtained by PCA(linear) and AutoEncoder(arbitrary)
#+END_LARGER
#+END_CENTER

#+BEGIN_ALIGNRIGHT
How to use the latent space?
#+END_ALIGNRIGHT

* Reinforcement Learning

*Policy function* $\pi(s)\mapsto a : S \rightarrow A$ -- returns action $a$ for state $s$

Agent always follows the policy function

*Optimal Policy* $\pi^* (s)$ : a policy that gives the highest reward

Goal: *Find/learn* the best approximation of $\pi^*$

Methods: Value-iteration, Policy-iteration, TD-learning ∋ Q-learning

* Reinforcement Learning(RL) in Latent Space (e.g. Luck IROS14, AAAI16)

RL in $S$ is too difficult → Apply RL to $L$ for speedup ($L$ is obtained by PCA)

\[x\in S, z \in L: \ x = f(z) \]

　

mapping 62-DOF space → 2D

[[png:static/latent-RL]]

# * Underlying belief:
# 
# #+BEGIN_QUOTE
# The real world is apparently complex, but in *almost all cases* they can be described by *only a few parameters*.
# #+END_QUOTE

* Deep Reinforcement Learning: DQN

No, DQN is not using latent space representation

DQN represents policy-function (state->action)

but it is possible


* Classical Planning 

_/✔/_ *Scalable, Highly-optimized solver* for complex combinatorial problems

_/✔/_ Guided by *domain-independent* heuristics

*/✘/* *Requires an explicit encoding* of the real world, written by human

* Comparison

#+BEGIN_CONTAINER-FLUID
#+BEGIN_ROW-FLUID
#+BEGIN_SPAN6
#+BEGIN_CENTER
*Latent Reinforcement Learning*
#+END_CENTER

_/✔/_ Works on the *implicit encoding* of the real world

*/✘/* Reasoning is limited to the *1-step future* of the current state

*/✘/* guided by *instance-specific learned knowledge* (specific object, situation, goal)
#+END_SPAN6
#+BEGIN_SPAN6
#+BEGIN_CENTER
*Classical Planning*
#+END_CENTER

_/✔/_ *Scalable, Highly-optimized solver* for complex combinatorial problems

_/✔/_ Guided by *domain-independent* heuristics

*/✘/* *Requires an explicit encoding* of the real world, written by human
#+END_SPAN6
#+END_ROW-FLUID
#+END_CONTAINER-FLUID

* Goal of this project

|                      |                        |                        |
|----------------------+------------------------+------------------------|
| Man-made             |                        |                        |
| state representation | Reinforcement Learning | Classical Planning     |
|----------------------+------------------------+------------------------|
| Latent state         | Latent RL              | *Latent Planning*      |
| (PCA)                |                        |                        |
|----------------------+------------------------+------------------------|
| Deep Latent state    |                        | *Deep Latent Planning* |
| (Deep AE)            |                        |                        |

* How?

Simply put: discretize $z$ into SAS variables

Below: results of encoding a MNIST image (784-variable) to 2 variables

Mapping the input image to latent space (encoding part)

[[png:static/vae-latent]]

* How?

Mapping the latent space back to the actual image (decoding part)

[[png:static/vae-manifold]]

* Action Representation

[[png:action2]]

* Action Representation (high-dimensional)

[[png:action]]



* System Overview

[[png:overview]]

